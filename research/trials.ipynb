{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1bd2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\patel\\\\OneDrive\\\\Desktop\\\\Projects\\\\CareBot\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a300d747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\patel\\\\OneDrive\\\\Desktop\\\\Projects\\\\CareBot'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73d480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patel\\anaconda3\\envs\\carebot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5563ef7",
   "metadata": {},
   "source": [
    "Extracting the Data (Gale Encyclopedia of Medicine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3125fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                              glob='*.pdf',\n",
    "                              loader_cls = PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f58ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(data = 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a024d390",
   "metadata": {},
   "source": [
    "Chunking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2125173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_chunking(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9431c99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text chunks: 5859\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_chunking(extracted_data)\n",
    "print('Length of the text chunks:', len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79b2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def donwload_huggingface_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name= 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f103d885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_20588\\2724969126.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name= 'sentence-transformers/all-MiniLM-L6-v2')\n",
      "c:\\Users\\patel\\anaconda3\\envs\\carebot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = donwload_huggingface_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371ff449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length : 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello World!\")\n",
    "print('length :' , len(query_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512f2b4",
   "metadata": {},
   "source": [
    "Creating Indexes using pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b04a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key = PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"carebot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric = 'cosine',\n",
    "        spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# address the dimension dynamicc....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11825745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "load_dotenv()\n",
    "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
    "# OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "# os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff94274",
   "metadata": {},
   "source": [
    "Creating Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e13d8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vs = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeea547c",
   "metadata": {},
   "source": [
    "Loading the Vector Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87200edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x150bed36050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "vs = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc2b23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vs.as_retriever(search_type='similarity', search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d54fa8",
   "metadata": {},
   "source": [
    "Trial of the search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d10e622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='783a6e47-cff9-45f3-8035-716e4e8ed06e', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='b6f05515-2c5e-4d56-842b-d2b743ef9173', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 37.0, 'page_label': '38', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='Acidosis see Respiratory acidosis; Renal\\ntubular acidosis; Metabolic acidosis\\nAcne\\nDefinition\\nAcne is a common skin disease characterized by\\npimples on the face, chest, and back. It occurs when the\\npores of the skin become clogged with oil, dead skin\\ncells, and bacteria.\\nDescription\\nAcne vulgaris, the medical term for common acne, is\\nthe most common skin disease. It affects nearly 17 million\\npeople in the United States. While acne can arise at any'),\n",
       " Document(id='4d8e5c8a-6188-4381-8864-3c450f90fd2f', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 38.0, 'page_label': '39', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': 'data\\\\Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a womanâ€™s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke('What is back Acne?')\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296eb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d7a5af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_20588\\195014683.py:35: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain_huggingface.chat_models.huggingface import ChatHuggingFace\n",
    "# from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "# huggingface_repo_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "# def load_llm(huggingface_repo_id):\n",
    "#     llm = HuggingFaceEndpoint(\n",
    "#         repo_id = huggingface_repo_id,\n",
    "#         huggingfacehub_api_token=HF_TOKEN,\n",
    "#         task='conversational',  \n",
    "#         max_new_tokens = 256,\n",
    "#         temperature = 0.3           \n",
    "#     )\n",
    "#     return llm\n",
    "\n",
    "# chat = ChatHuggingFace(llm = load_llm(huggingface_repo_id))\n",
    "\n",
    "#genai\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",       # or \"gemini-1.5-flash\" if you want it faster/cheaper\n",
    "    temperature=0.3,\n",
    "    max_output_tokens=512,\n",
    "    google_api_key=GOOGLE_API_KEY  # or rely on env var only\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",   # this key will be used in the prompt\n",
    "    return_messages=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff812376",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =\"\"\"You are an assistant for the question answer tasks. Use the following pieces of retrieved context to answer the question. If you don't \n",
    "    know the asnwer, say that you don't know. Don't provide anything out of the given context. Use three sentences maximum and keep the answer concise. \\n\\n\n",
    "    \n",
    "    Chat history: {chat_history}\n",
    "    Context : {context}\n",
    "    Question : {question}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=system_prompt,\n",
    "    input_variables=[\"chat_history\", \"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29ddc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "conv_rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    "    get_chat_history=lambda h: h  # h is a list of messages; we pass it straight to {chat_history}\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "# model = load_llm()\n",
    "\n",
    "# def get_answer(query, context):\n",
    "#     \"\"\"Function to get answer from the model using context\"\"\"\n",
    "#     prompt = f\"\"\"Context: {context}\n",
    "    \n",
    "# Question: {query}\n",
    "\n",
    "# Answer the question based on the context above. Keep it concise and within 3 sentences. If you can't find the answer in the context, say \"I don't know\".\"\"\"\n",
    "    \n",
    "#     response = model.generate_content(prompt)\n",
    "#     return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e3a2a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_46188\\2152260917.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Temp\\ipykernel_46188\\2152260917.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "query = \"What is acne?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "try:\n",
    "    answer = get_answer(query, context)\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# system_prompt = (\n",
    "#     \"You are an assistant for the question answer tasks. Use the following pieces of retrieved context to answer the question. If you don't \" \\\n",
    "#     \"know the asnwer, say that you don't know. Use three sentences maximum and keep the answer concise. \\n\\n\"\n",
    "#     \"{context}\"\n",
    "# )\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         ('system', system_prompt),\n",
    "#         ('human', '{input}')\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7622f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "# llm = Ollama(model = 'mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c4f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for the question answer tasks. Use the following pieces of retrieved context to answer the question. If you don't know the asnwer, say that you don't know. Use three sentences maximum and keep the answer concise. \\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
      "| Ollama(model='mistral')\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n"
     ]
    }
   ],
   "source": [
    "# question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# print(question_answer_chain)\n",
    "# rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acne is a common skin disease characterized by pimples on the face, chest, and back. It occurs when the pores of the skin become clogged with oil, dead skin cells, and bacteria. Acne vulgaris, also known as common acne, is the most prevalent form and affects nearly 17 million people in the U.S.\n"
     ]
    }
   ],
   "source": [
    "# response = rag_chain.invoke({'input': 'What is Acne'})\n",
    "# print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defbe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The treatment for back acne can involve topical drugs like tretinoin, benzoyl peroxide, adapalene, or salicylic acid to reduce the formation of new comedones. Shampooing often, wearing hair off the face, avoiding foods that trigger flare-ups, and reducing stress are also recommended. Additionally, alternative treatments focus on proper cleansing, a balanced diet high in fiber, zinc, and raw foods, and avoiding certain triggers like alcohol and processed foods.\n"
     ]
    }
   ],
   "source": [
    "# response = rag_chain.invoke({'input': 'What is cure for back acne?'})\n",
    "# print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e4bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry for any confusion, but the context provided doesn't mention the Samsung Odyssey G7 or its price. It seems to be discussing the cost of Alexander technique lessons, bone grafting procedures, and appendectomy-related costs. If you need help finding the price of Samsung Odyssey G7, I would recommend checking electronics retailers such as Best Buy, Amazon, or directly on Samsung's official website.\n"
     ]
    }
   ],
   "source": [
    "# response = rag_chain.invoke({'input': 'How much does samsung odyssey g7 cost?'})\n",
    "# print(response['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carebot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
